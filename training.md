## ELK basic training

**If you're using a dedicated VM replace localhost by your VM IP.**

###Â Step 1: Understanding Elasticsearch

For our first steps withi ELK we will work with a very common and comprehensive dataset: a movie list from IMDB.

1. Using Postman or curl insert your first document in the movies index, you should have a positive response.

  ```
  $ curl -X POST --data '{"title": "Star Wars", "director": "George Lucas", "country": "usa", "year": 1977, "duration": 140, "rating": 8.2}' http://localhost:9200/movies/movie/1
  ```

1. You can now retrieve this document from the movie index using the _search API:

  ```
  $ curl http://localhost:9200/movies/_search
  ```

1. A single document is not a funny case of study... Let's add more document to the index ! You can do it manually or
can use the bulk API to insert a whole dataset to the index.
  ```
  $ curl -X POST --data-binary @datasets/imdb/movie_metadata.bulk http://localhost:9200/movies/movie/_bulk 
  ```

If you've made a mistake you can easily remove documents or the entire index

```
curl -X DELETE http://localhost:9200/movies
```

1. Now with the help of  [search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html),
[aggregation API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html) and
the [Lucene syntax reference](https://lucene.apache.org/core/2_9_4/queryparsersyntax.html)
you can try to make more complex request. For example:

  1. List all movies from a director named George
  1. List all french movies that has a rating over 8.0
  1. Compute the average rating of 2001

### Step 2: Understanding Kibana
You might have noticed that querying manually elasticsearch might be tedious... It's now a good time to introduce Kibana !

1. Connect to the Kibana web UI at `localhost:5601`
1. The first time you get connected you will to define an index mapping. Fill in `movies` as pattern and untick the
checkbox `Index contains time-based event`.
1. From the discover view you can browse the dataset and make requests like you did using the _search API
1. Now let's use the real power of Kibana. Click on the "Visualize" tab and try to build some visualisation, for example:
  1. Average duration of movies
  1. Rating Distribution
  1. Average rating depending on the year
  1. Production share by country
  1. Top 10 of the countries that have the best rating average
1. Finally wrap all your visualiation into a single dashboard

### Step 4: Indexation through logstash

1. Take a look at the `conf/logstash/logstash-uber.conf` file to understand the input format
1. You can try to push manually a log entry
```
telnet localhost 5000
"16/1/2016 2:33:00",40.7445,-73.9855,"B02512"
```
If you have an eye on the Logstash logs you should see that your message has been processed by Logstash


### Step 5: Working a real use case

1. Use the `streamer.py` script to stream the entire data set through logstash 
  ```
  $ cd datasets/uber
  $ python streamer.py &
  ```
1. In Kibana configure the new index logstash-uber, you should be able to see the new incoming data. The dataset
is streamed on a period of one month, so you should adjust the time window accordingly.
1. Use the `upload.sh` script to upload the existing uber dashboard
  ```
  $ cd dashboards/uber/script
  $ python upload.py
  ```
1. From the dashboard tab you should be able to display it
1. Now that you are an accomplished ELK developer, Uber needs you ! 


> Uber management would like to know when the users are requesting the more a ride within a day. This information would
> useful to adjust the size of the fleet.
> Someone in the audience also made the
> observation that user's behavior might be different depending on the day of the week, especially between the
> week days and the weekend.

The data ingest team has already added a new "timeperiod" field that you should be able to see in Kibana.
You should have everything you need to build this new vizualisation and make Uber managers super happy.
